{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOfqTdth1bUB+jOvGioroS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadwin-357/ProteinMPNN_breakdown/blob/main/model_utils_function_8_Encoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test model utils function\n",
        "#Encoder - decoder\n",
        "Encoder:\n",
        " 1.take h_E (edge feature) (postional + geometry) and h_V(initialize with zero -1 dimension is the same as h_E)\\\n",
        " 2.concat: neighbor nodes embedding and edge embedding to h_EV  [B,N,K,C+C]\\\n",
        "3.concat: h_V with h_EV (center nodes + neigher nodes\n",
        "and edges) [B,N,K,C+C+C]\n",
        "4. MLP: three layers of MLP and Activation to generate h_message [B,N,K, num_hidden]\n",
        "5. updata h_message with attention_mask if there is one   [B,N,K,num_hidden]\n",
        "5. sum over the K dimension and scaled by a scaling factor -> dh [B,N,num_hidden]\n",
        "6. norm(dropout(dh) + h_V) --> h_V (as a residual ?)\n",
        "7. MLP(dh) ->dh  [B,N,num_hidden]\n",
        "8. norm(h_V +dropout(dh)) --> udpate h_V  [B, N, num_hidden]\n",
        "9. add mask_V if there is one -> h_V\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The high-level summary: collect neighbor embedding, combine self embeddeing, Go through MLP to form message;\n",
        "The following is how the message is used to update the h_V node embedding\\\n",
        "1.sum over message by -2 dimension and add the result to h_V\n",
        "2.MLP through the summarized message and add the reuslt to h_V again\n",
        "\n",
        "similarly, the h_E will be updated through the message.\n",
        "In each layer, the update h_V and h_E will be used to update each other in a evolving way\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Decoder:\n",
        "Similar to Encoder, but only h_V will be updated\n",
        "the upated h_V will be used for prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "oqqLnTEsr3Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class EncLayer(nn.Module):\n",
        "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
        "        super(EncLayer, self).__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_in = num_in\n",
        "        self.scale = scale\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(num_hidden)\n",
        "        self.norm2 = nn.LayerNorm(num_hidden)\n",
        "        self.norm3 = nn.LayerNorm(num_hidden)\n",
        "\n",
        "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
        "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.W11 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
        "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.W13 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.act = torch.nn.GELU()\n",
        "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
        "\n",
        "    def forward(self, h_V, h_E, E_idx, mask_V=None, mask_attend=None):\n",
        "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
        "\n",
        "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
        "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
        "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
        "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
        "        if mask_attend is not None:\n",
        "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
        "        dh = torch.sum(h_message, -2) / self.scale\n",
        "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
        "\n",
        "        dh = self.dense(h_V)\n",
        "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
        "        if mask_V is not None:\n",
        "            mask_V = mask_V.unsqueeze(-1)\n",
        "            h_V = mask_V * h_V\n",
        "\n",
        "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
        "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
        "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
        "        h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))\n",
        "        h_E = self.norm3(h_E + self.dropout3(h_message))\n",
        "        return h_V, h_E"
      ],
      "metadata": {
        "id": "51TvPerj4fW-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n",
        "    h_nodes = gather_nodes(h_nodes, E_idx)  # B, N, K, C\n",
        "    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n",
        "    return h_nn"
      ],
      "metadata": {
        "id": "xMSihDDE5Zu1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# h_V: node embedding [B,N,C]h_E edge_embedding [B,N,K,C], E_idx: neighbor idx\n",
        "\n",
        "h_EV = cat_neighbors_nodes(h_V, h_E, E_idx) # combine neighbor node and edge embedding\n",
        "h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)  [B,N,k,C]\n",
        "h_EV = torch.cat([h_V_expand, h_EV], -1) # add self node embedding\n",
        "h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV))))) # forming message\n",
        "if mask_attend is not None:\n",
        "    h_message = mask_attend.unsqueeze(-1) * h_message\n",
        "dh = torch.sum(h_message, -2) / self.scale # sum over -2 dimension (neighbors)\n",
        "h_V = self.norm1(h_V + self.dropout1(dh)) # use summarized message to update h_V\n",
        "\n",
        "dh = self.dense(h_V)\n",
        "h_V = self.norm2(h_V + self.dropout2(dh))\n",
        "if mask_V is not None:\n",
        "    mask_V = mask_V.unsqueeze(-1)\n",
        "    h_V = mask_V * h_V\n",
        "\n",
        "h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
        "h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
        "h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
        "h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))\n",
        "h_E = self.norm3(h_E + self.dropout3(h_message)) # use summarized message to update h_E\n",
        "'''"
      ],
      "metadata": {
        "id": "SxTWbT335Lxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder"
      ],
      "metadata": {
        "id": "xJOI4VTi689H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecLayer(nn.Module):\n",
        "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
        "        super(DecLayer, self).__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_in = num_in\n",
        "        self.scale = scale\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(num_hidden)\n",
        "        self.norm2 = nn.LayerNorm(num_hidden)\n",
        "\n",
        "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
        "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.act = torch.nn.GELU()\n",
        "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
        "\n",
        "    def forward(self, h_V, h_E, mask_V=None, mask_attend=None):\n",
        "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
        "\n",
        "        # Concatenate h_V_i to h_E_ij\n",
        "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_E.size(-2),-1)\n",
        "        h_EV = torch.cat([h_V_expand, h_E], -1)\n",
        "\n",
        "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
        "        if mask_attend is not None:\n",
        "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
        "        dh = torch.sum(h_message, -2) / self.scale\n",
        "\n",
        "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
        "\n",
        "        # Position-wise feedforward\n",
        "        dh = self.dense(h_V)\n",
        "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
        "\n",
        "        if mask_V is not None:\n",
        "            mask_V = mask_V.unsqueeze(-1)\n",
        "            h_V = mask_V * h_V\n",
        "        return h_V"
      ],
      "metadata": {
        "id": "33UguUQ67JYy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_E.size(-2),-1)  # expand [B,N,C] -> [B,N,K,C]\n",
        "h_EV = torch.cat([h_V_expand, h_E], -1) # add edge embeddign to h_V\n",
        "\n",
        "h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV))))) # form message\n",
        "if mask_attend is not None:\n",
        "    h_message = mask_attend.unsqueeze(-1) * h_message\n",
        "dh = torch.sum(h_message, -2) / self.scale  # sum message over neighbors\n",
        "\n",
        "h_V = self.norm1(h_V + self.dropout1(dh)) # used summarized message to update h_V\n",
        "\n",
        "# Position-wise feedforward\n",
        "dh = self.dense(h_V)\n",
        "h_V = self.norm2(h_V + self.dropout2(dh))\n",
        "\n",
        "if mask_V is not None:\n",
        "    mask_V = mask_V.unsqueeze(-1)\n",
        "    h_V = mask_V * h_V\n",
        "return h_V\n",
        "'''"
      ],
      "metadata": {
        "id": "cdLq588k7M6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}